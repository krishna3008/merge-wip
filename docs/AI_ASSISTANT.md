# AI Debugging Assistant - User Guide

## Overview

Merge Assist includes an **AI Debugging Assistant** powered by OpenAI GPT-4 that provides intelligent insights for merge requests, pipeline failures, and troubleshooting.

---

## Features

### 1. Merge Conflict Analysis
AI analyzes merge conflicts and provides:
- Root cause analysis
- Step-by-step resolution suggestions
- Severity assessment (low/medium/high)
- Auto-resolvability determination

### 2. Pipeline Failure Diagnosis
When pipelines fail, AI provides:
- Root cause identification
- Fix suggestions
- Estimated fix time
- Links to similar issues

### 3. Code Review Focus
AI suggests:
- Key areas for reviewers to focus on
- Risk assessment
- Estimated review time
- Recommended automated checks

### 4. Batch Merge Optimization
AI optimizes batch groupings by:
- Analyzing MR relationships
- Minimizing conflict potential
- Grouping related changes
- Estimating success rates

### 5. Stuck MR Diagnosis
For stuck MRs, AI provides:
- Diagnosis of the issue
- Probable causes
- Recommended actions
- Manual intervention assessment

### 6. Merge Summaries
AI generates professional summaries of batch merges for:
- Release notes
- Team notifications
- Change logs

---

## Setup

### 1. Obtain OpenAI API Key

```bash
# Sign up at https://platform.openai.com
# Create API key in dashboard
# Copy the key (starts with 'sk-...')
```

### 2. Store API Key in Secrets Manager

#### Option A: AWS Secrets Manager (Production)

```bash
aws secretsmanager create-secret \
    --name merge-assist/openai/api_key \
    --secret-string '{"api_key":"sk-your-openai-api-key-here"}'
```

#### Option B: Local Encrypted File (Development)

```bash
python << EOF
from backend.secrets.secrets_manager import SecretsManager
import os

os.environ['SECRETS_PROVIDER'] = 'local'
os.environ['SECRETS_FILE'] = '/app/secrets/local_secrets.enc'
os.environ['MASTER_PASSWORD'] = 'your-master-password'

manager = SecretsManager()
manager.initialize()

# Store OpenAI API key
manager._provider.set_secret('openai/api_key', {
    'api_key': 'sk-your-openai-api-key-here'
})
EOF
```

### 3. Enable AI in Worker POD

The AI assistant automatically activates when the API key is available. No code changes needed!

---

## Usage

### Automatic Features

AI assistance is **automatically triggered** in these scenarios:

1. **Merge Conflicts Detected**
   - AI analyzes conflicts when MR has `has_conflicts: true`
   - Posts analysis as comment on MR
   
2. **Pipeline Failure**
   - AI diagnoses failure when pipeline status is `failed`
   - Posts diagnosis with fix suggestions as comment

3. **Batch Merge**
   - AI optimizes MR groupings when â‰¥5 MRs are ready
   - Selects optimal batches for merge

4. **Stuck MR Detection**
   - AI diagnoses MRs in same status for >1 hour
   - Posts diagnostic report as comment

### Manual Invocation (via API)

You can manually request AI insights:

```bash
# Analyze specific MR conflicts
curl -X POST https://your-api/ai/analyze-conflicts \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "mr_id": "uuid-of-mr",
    "conflict_files": ["backend/models.py", "backend/api.py"]
  }'

# Diagnose stuck MR
curl -X POST https://your-api/ai/diagnose-stuck \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"mr_id": "uuid-of-mr"}'

# Request review suggestions
curl -X POST https://your-api/ai/review-focus \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "mr_id": "uuid-of-mr",
    "description": "Add new authentication feature",
    "changed_files": ["auth.py", "routes.py"],
    "lines_added": 150,
    "lines_deleted": 20
  }'
```

---

## Example AI Outputs

### Merge Conflict Analysis

```markdown
## ðŸŸ¡ AI Conflict Analysis

**Severity:** MEDIUM

### Analysis
The conflicts in backend/database/models.py suggest two parallel feature 
developments modified the User model. Both branches added new fields to the 
same class, resulting in overlapping changes.

### Suggested Resolution Steps
1. Review both sets of changes to understand intent
2. Merge both field additions, ensuring no duplicate names
3. Update migration scripts to include both changes
4. Run tests to verify model integrity
5. Update API documentation if new fields are exposed

âš ï¸ **Note:** Manual resolution required.

*This analysis was generated by Merge Assist AI.*
```

### Pipeline Failure Diagnosis

```markdown
## ðŸ” AI Pipeline Failure Diagnosis

**Pipeline ID:** 12345

### Root Cause
The test:integration job failed due to a database connection timeout. This 
typically occurs when the test database is not available or network issues 
prevent connection establishment.

### Suggested Fixes
1. Verify test database is running and accessible
2. Check DATABASE_URL environment variable in CI configuration
3. Increase connection timeout in test configuration
4. Ensure database migrations run before tests
5. Check for concurrent test execution causing lock issues

**Estimated Fix Time:** quick

### Related Issues
- Issue #789: Similar timeout in integration tests
- GitLab CI Docs: Database service configuration

*This diagnosis was generated by Merge Assist AI.*
```

### Stuck MR Diagnostic

```markdown
## ðŸ”§ AI Diagnostic Report: Stuck MR

### Diagnosis
The MR has been in 'ready' status for 2 hours without progression. The 
pipeline completed successfully, but the merge was not triggered. This 
suggests the Worker POD may not be processing events or there's a 
configuration issue preventing automatic merge.

### Probable Causes
1. Worker POD for this project is not running
2. Redis pub/sub event not delivered
3. MR doesn't meet all merge criteria (approvals, discussions)
4. Manual merge protection enabled

### Recommended Actions
1. Check Worker POD logs: kubectl logs -l project=your-project
2. Verify Redis connection: redis-cli ping
3. Check MR approvals and discussion status
4. Manually trigger merge check via API
5. Restart Worker POD if necessary

âš ï¸ **Manual intervention required**

*This diagnostic was generated by Merge Assist AI.*
```

---

## Configuration

### AI Model Selection

Default: `gpt-4` (best reasoning)

To use GPT-3.5 (faster, cheaper):

```python
# In backend/ai/ai_assistant.py
self.model = "gpt-3.5-turbo"
```

### Temperature Settings

- **Conflict Analysis**: 0.3 (consistent, technical)
- **Pipeline Diagnosis**: 0.3 (precise, factual)
- **Review Suggestions**: 0.3 (systematic)
- **Batch Optimization**: 0.4 (creative grouping)
- **Merge Summaries**: 0.5 (natural language)

### Cost Management

**Estimated costs** (GPT-4 pricing):
- Conflict analysis: ~$0.01-0.03 per analysis
- Pipeline diagnosis: ~$0.01-0.02 per diagnosis
- Batch optimization: ~$0.02-0.04 per batch
- MR diagnosis: ~$0.01-0.02 per diagnosis

**Monthly estimate** (100 MRs):
- ~20 conflicts: $0.40
- ~30 pipeline failures: $0.45
- ~15 batch optimizations: $0.45
- ~10 stuck MRs: $0.15
- **Total: ~$1.45/month**

### Rate Limiting

OpenAI has rate limits:
- **GPT-4**: 3 requests/minute (Tier 1)
- **GPT-3.5**: 60 requests/minute

Worker POD automatically handles rate limits with retries.

---

## Disabling AI

AI can be disabled without changing code:

1. **Remove API key from secrets**:
   ```bash
   aws secretsmanager delete-secret --secret-id merge-assist/openai/api_key
   ```

2. **Or set empty key**:
   ```bash
   manager._provider.set_secret('openai/api_key', {'api_key': ''})
   ```

AI Assistant will detect missing/empty key and disable automatically. Worker POD continues normal operation without AI features.

---

## Privacy & Security

### What Data is Sent to OpenAI?

AI submits only:
- MR titles
- Branch names
- File paths (not contents)
- Error messages
- Pipeline status

**Never sent:**
- Source code
- API tokens
- Secrets
- User passwords
- Internal URLs

### Data Retention

Per OpenAI's policy:
- API requests not used for training (Enterprise tier)
- Data retained 30 days for abuse monitoring
- Can request zero retention with Enterprise plan

### Compliance

For regulated environments:
- Use Azure OpenAI (HIPAA/SOC2 compliant)
- Or disable AI features
- Or use on-premises LLM (requires custom integration)

---

## Troubleshooting

### AI Features Not Working

```bash
# Check if API key is configured
kubectl logs -l app=worker | grep "AI assistant"

# Expected:
# "AI assistant enabled with model gpt-4"

# If disabled:
# "OpenAI API key not found, AI assistant disabled"
```

### API Rate Limit Errors

```bash
# Reduce AI calls by increasing thresholds
# Only analyze conflicts with >3 files:
if len(conflict_files) > 3:
    analysis = await ai.analyze_merge_conflict(...)
```

### Unexpected AI Responses

- Lower temperature (0.1-0.2) for more deterministic output
- Add more context to prompts
- Switch to GPT-4 for better reasoning
- Check token limits (max_tokens)

---

## Future Enhancements

Planned AI features:
- **Predictive Success Rates**: Predict merge success probability
- **Automated Code Review**: Full code review with suggestions
- **Conflict Auto-Resolution**: Automatically resolve simple conflicts
- **Learning from History**: Improve suggestions based on past resolutions
- **Multi-Language Support**: AI responses in user's preferred language

---

*For technical details, see `backend/ai/ai_assistant.py`*
